{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Copyright **`(c)`** 2022 Giovanni Squillero `<squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Lab 3: Policy Search\n",
    "\n",
    "## Task\n",
    "\n",
    "Write agents able to play [*Nim*](https://en.wikipedia.org/wiki/Nim), with an arbitrary number of rows and an upper bound $k$ on the number of objects that can be removed in a turn (a.k.a., *subtraction game*).\n",
    "\n",
    "The player **taking the last object wins**.\n",
    "\n",
    "* Task3.1: An agent using fixed rules based on *nim-sum* (i.e., an *expert system*)\n",
    "* Task3.2: An agent using evolved rules\n",
    "* Task3.3: An agent using minmax\n",
    "* Task3.4: An agent using reinforcement learning\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* Create the directory `lab3` inside the course repo \n",
    "* Put a `README.md` and your solution (all the files, code and auxiliary data if needed)\n",
    "\n",
    "## Notes\n",
    "\n",
    "* Working in group is not only allowed, but recommended (see: [Ubuntu](https://en.wikipedia.org/wiki/Ubuntu_philosophy) and [Cooperative Learning](https://files.eric.ed.gov/fulltext/EJ1096789.pdf)). Collaborations must be explicitly declared in the `README.md`.\n",
    "* [Yanking](https://www.emacswiki.org/emacs/KillingAndYanking) from the internet is allowed, but sources must be explicitly declared in the `README.md`.\n",
    "\n",
    "**Deadline**\n",
    "\n",
    "T.b.d.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import functools\n",
    "from typing import Callable\n",
    "from itertools import accumulate\n",
    "from copy import deepcopy\n",
    "from operator import xor\n",
    "from collections import namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "TUNING = False\n",
    "CODE_FOR_TABLE = True\n",
    "RL_TRAINING = False\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nimply = namedtuple(\"Nimply\", \"row, num_objects\")\n",
    "Move = namedtuple(\"Move\", \"row num_objects fitness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NIM Game "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nim:\n",
    "    def __init__(self, num_rows: int, k: int = None) -> None:\n",
    "        self._rows = [i*2 + 1 for i in range(num_rows)]\n",
    "        self._k = k\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self._rows}\"\n",
    "\n",
    "    def nimming(self, row: int, num_objects: int) -> None:\n",
    "        assert self._rows[row] >= num_objects\n",
    "        assert self._k is None or num_objects <= self._k\n",
    "        assert num_objects > 0, f\"You have to pick at least one\"\n",
    "        self._rows[row] -= num_objects\n",
    "        if sum(self._rows) == 0:\n",
    "            logging.debug(\"Yeuch\")\n",
    "    \n",
    "    def nimming2(self, ply: Nimply) -> None:\n",
    "        row, num_objects = ply\n",
    "        assert self._rows[row] >= num_objects\n",
    "        assert self._k is None or num_objects <= self._k\n",
    "        assert num_objects > 0, f\"You have to pick at least one\"\n",
    "        self._rows[row] -= num_objects\n",
    "\n",
    "    @property\n",
    "    def rows(self):\n",
    "        return self._rows\n",
    "\n",
    "    @property\n",
    "    def k(self) -> int:\n",
    "        return self._k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nim_sum(rows: list) -> int:\n",
    "    # List XOR\n",
    "    # Using reduce() + lambda + \"^\" operator\n",
    "    res = functools.reduce(lambda x, y: x ^ y, rows)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tournament(population, tournament_size=2):\n",
    "    return min(random.choices(population, k=tournament_size), key=lambda i: i.fitness)\n",
    "\n",
    "def mutation(p: Move, nim: Nim):\n",
    "    if nim.k is None:\n",
    "        elements = random.randrange(1, nim.rows[p.row] + 1)\n",
    "        temp_rows = nim.rows.copy()\n",
    "        temp_rows[p.row] -=elements \n",
    "        offspring = Move(p.row, elements , nim_sum(temp_rows))\n",
    "    else:\n",
    "        elements = min(nim.k, random.randrange(1, nim.rows[p.row] + 1))\n",
    "        temp_rows = nim.rows.copy()\n",
    "        temp_rows[p.row] -=elements \n",
    "        offspring = Move(p.row, elements , nim_sum(temp_rows))\n",
    "\n",
    "    return offspring\n",
    "\n",
    "def cross_over(p1: Move, p2: Move, nim: Nim):\n",
    "\n",
    "    n_random = random.randint(0, 1)\n",
    "    \n",
    "    if n_random == 0:\n",
    "\n",
    "        temp_rows = nim.rows.copy()\n",
    "        temp_rows[p1.row] -= p2.num_objects\n",
    "\n",
    "        if temp_rows[p1.row] < 0:\n",
    "            return None\n",
    "\n",
    "        offspring = Move(p1.row, p2.num_objects , nim_sum(temp_rows))\n",
    "    else:\n",
    "        temp_rows = nim.rows.copy()\n",
    "        temp_rows[p2.row] -= p1.num_objects\n",
    "\n",
    "        if temp_rows[p2.row] < 0:\n",
    "            return None\n",
    "\n",
    "        offspring = Move(p2.row, p1.num_objects , nim_sum(temp_rows))\n",
    "    \n",
    "    if nim.k is not None and offspring.num_objects > nim.k:\n",
    "        return None\n",
    "\n",
    "    return offspring\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the NIM Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ROWS = 3\n",
    "GAMEOVER = [0 for _ in range(N_ROWS)]\n",
    "POPULATION_SIZE = 10\n",
    "OFFSPRING_SIZE = 10\n",
    "N_GENERATIONS = 20\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cook_status(state: Nim) -> dict:\n",
    "    cooked = dict()\n",
    "    cooked[\"possible_moves\"] = [\n",
    "        (r, o) for r, c in enumerate(state.rows) for o in range(1, c + 1) if state.k is None or o <= state.k\n",
    "    ]\n",
    "    cooked[\"active_rows_number\"] = sum(o > 0 for o in state.rows)\n",
    "    cooked[\"shortest_row\"] = min((x for x in enumerate(state.rows) if x[1] > 0), key=lambda y: y[1])[0]\n",
    "    cooked[\"longest_row\"] = max((x for x in enumerate(state.rows)), key=lambda y: y[1])[0]\n",
    "    cooked[\"nim_sum\"] = nim_sum(state.rows)\n",
    "\n",
    "    brute_force = list()\n",
    "    for m in cooked[\"possible_moves\"]:\n",
    "        tmp = deepcopy(state)\n",
    "        tmp.nimming2(m)\n",
    "        brute_force.append((m, nim_sum(tmp.rows)))\n",
    "    cooked[\"brute_force\"] = brute_force\n",
    "\n",
    "    possible_new_states = list()\n",
    "    for m in cooked[\"possible_moves\"]:\n",
    "            tmp = deepcopy(state)\n",
    "            tmp.nimming2(m)\n",
    "            # (state, move to reach the state)\n",
    "            possible_new_states.append((tmp, m))\n",
    "    cooked[\"possible_new_states\"] = possible_new_states\n",
    "\n",
    "    return cooked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Player:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategies:\n",
    "\n",
    "* Task 3.1 (_Expert System_) : _Best Possibile Strategy_ implemented in `best_strategy` and nominated as `best` (see below). To understand the algorithm of the winning strategy, look at [*Nim*](https://en.wikipedia.org/wiki/Nim)!\n",
    "* Task 3.1 Bis (_Expert System_) : _Fixed Rules I would play_ implemented in `evolvable_based_on_fixed_rules`  but using $\\alpha = 0.5$ and $\\beta = 0.5$ and nominated as `evolvable`.\n",
    "* Task 3.2 (_Evolvable Strategies_): _Evolvable Strategies based on GA_ implemented in `evolvable_based_on_GA` and nominated as `ga`.\n",
    "* Task 3.2 bis (_Evolvable Strategies_): _Evolvable Strategies based on Fixed Rules_ implemented in `evolvable_based_on_fixed_rules`  but using $\\alpha = 0.4$ and $\\beta = 0.1$ (best parameters found when playing against a pure random opponent and using a random $K$) and nominated as `evolvable`.\n",
    "* Task 3.3: _Minimax Strategy_ implemented in `min_max_best_move` and nominated as `min_max`, code based on [MinMax](https://realpython.com/python-minimax-nim/).\n",
    "* Task 3.4: _RL agent_ implemented in `rl_agent` and nominated as `rl`.\n",
    "* Other strategies:\n",
    "  * Best strategy By [Prof. Squillero](https://github.com/squillero) implemented in `best_strategy_by_prof` and nominated as `best_prof`.\n",
    "  * Evolvable strategy By [Prof. Squillero](https://github.com/squillero) implemented in `evolvable_by_prof` and nominated as `evolvable_prof`.\n",
    "\n",
    "To use this strategies, use their nomination (e.g. `best`, `evolvable`, ... ) in the `evaulate` function below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    def __init__(self, strategy = 'best') -> None:\n",
    "        # Two parts for the best strategy:\n",
    "        # 0 -> before all rows have one element\n",
    "        # 1 -> after all rows have one element\n",
    "        self._best_strategy = 0\n",
    "\n",
    "        assert strategy in ['best', 'best_prof', 'pure_random', 'ga', 'evolvable', 'evolvable_prof', 'evolvable_tuned', 'min_max', 'rl'], f\"Strategy non-available\"\n",
    "        self._strategy = strategy\n",
    "        if strategy == 'rl':\n",
    "            self.agent = Agent(pretrained=True)\n",
    "\n",
    "    def moves(self, Nim, alpha = 0.5, beta = 0.5):\n",
    "        if self._strategy == 'best':\n",
    "            return self.best_strategy(Nim)\n",
    "        elif self._strategy == 'best_prof':\n",
    "            return self.best_strategy_by_prof(Nim)\n",
    "        elif self._strategy == 'pure_random':\n",
    "            return self.pure_random(Nim)\n",
    "        elif self._strategy == 'ga':\n",
    "            return self.evolvable_based_on_GA(Nim)\n",
    "        elif self._strategy == 'evolvable':\n",
    "            return self.evolvable_based_on_fixed_rules(Nim, cook_status(Nim), alpha, beta)\n",
    "        elif self._strategy == 'evolvable_tuned':\n",
    "            return self.evolvable_based_on_fixed_rules(Nim, cook_status(Nim), 0.4, 0.1)\n",
    "        elif self._strategy == 'evolvable_prof':\n",
    "            return self.evolvable_by_prof(Nim, alpha)\n",
    "        elif self._strategy == 'min_max':\n",
    "            return self.min_max_best_move(Nim)\n",
    "        elif self._strategy == 'rl':\n",
    "            return self.rl_agent(Nim)\n",
    "        else: \n",
    "            assert f\"Can't use a strategy\"\n",
    "        return\n",
    "\n",
    "\n",
    "    def pure_random(self, Nim):\n",
    "\n",
    "        # The opponent choose randomly a non-empty row \n",
    "        nonzeroind = np.nonzero(Nim.rows)[0]\n",
    "        random_row = random.choice(nonzeroind)\n",
    "\n",
    "        # The opponen choose to remove a random number of elements\n",
    "        if Nim._k == None:\n",
    "            random_elements = random.randint(1,Nim.rows[random_row])\n",
    "        else:\n",
    "            random_elements = random.randint(1,min(Nim._k,Nim.rows[random_row]))\n",
    "\n",
    "        return Nimply(random_row, random_elements)\n",
    "\n",
    "        \n",
    "    def best_strategy(self, Nim):\n",
    "\n",
    "        # If all the elements are equal or less then k, we can play the 'normal' nim game\n",
    "        if Nim._k != None and all(v <= Nim._k for v in Nim.rows):\n",
    "            temp_k = None\n",
    "        else:\n",
    "            temp_k = Nim._k\n",
    "\n",
    "        if temp_k != None:\n",
    "\n",
    "            # Try brute force:\n",
    "            for ind, row in enumerate(Nim.rows):\n",
    "\n",
    "                for el in range(1, min(row + 1, Nim._k + 1)):\n",
    "                    # Reset temp_rows\n",
    "                    temp_rows = Nim.rows.copy()\n",
    "                    \n",
    "                    # See if nim_sum == 0\n",
    "                    temp_rows[ind] -= el\n",
    "                    if nim_sum(temp_rows) == 0:\n",
    "                        # Update table\n",
    "                        # Nim.nimming(ind, el)\n",
    "                        return Nimply(ind, el)\n",
    "            \n",
    "            equal_grater_than_k_ind = [i for i,v in enumerate(Nim.rows) if v >= Nim._k + 1]\n",
    "            \n",
    "            random_row = random.choice(equal_grater_than_k_ind)\n",
    "            elements = Nim.rows[random_row]%(Nim._k+1) \n",
    "            \n",
    "            if elements == 0:\n",
    "                elements = 1\n",
    "\n",
    "            return Nimply(random_row, elements)\n",
    "\n",
    "        # If there is only one element greater to one, the agent picks a number of object to make\n",
    "        # all the rows of the table equal to 1.\n",
    "        # He can choose to remove all the objects or all the objects but one from the rows with n>1\n",
    "        if sum(x >= 2 for x in Nim.rows) == 1:\n",
    "            # Row with more than one element\n",
    "            equal_grater_than_two_ind = [i for i,v in enumerate(Nim.rows) if v >= 2][0]\n",
    "\n",
    "            # Change of strategy\n",
    "            self._best_strategy = 1\n",
    "\n",
    "            \n",
    "            # To win, the remaing number of objects has to be even \n",
    "            if (sum(x for x in Nim.rows) - Nim.rows[equal_grater_than_two_ind]) % 2 == 0 :\n",
    "        \n",
    "                return Nimply(equal_grater_than_two_ind, Nim.rows[equal_grater_than_two_ind])\n",
    "                \n",
    "            else:\n",
    "\n",
    "                return Nimply(equal_grater_than_two_ind, Nim.rows[equal_grater_than_two_ind]-1)        \n",
    "        \n",
    "        # Strategy before all rows have one element\n",
    "        if self._best_strategy == 0:    \n",
    "        \n",
    "            res = nim_sum(Nim.rows)\n",
    "\n",
    "            for ind, row in enumerate(Nim.rows):\n",
    "\n",
    "                if row == 0:\n",
    "                    continue\n",
    "\n",
    "                if row ^ res < row:\n",
    "                   \n",
    "                    elements = row - (row ^ res)\n",
    "\n",
    "                    return Nimply(ind, elements)\n",
    "        \n",
    "        # Strategy after all rows have one element\n",
    "        else:\n",
    "\n",
    "            nonzeroind = [i for i, e in enumerate(Nim.rows) if e != 0]\n",
    "            random_row = random.choice(nonzeroind)\n",
    "\n",
    "            return Nimply(random_row, 1) \n",
    "                 \n",
    "        # Default move -> Random\n",
    "        nonzeroind = [i for i, e in enumerate(Nim.rows) if e != 0]\n",
    "        random_row = random.choice(nonzeroind)\n",
    "\n",
    "        if Nim._k == None:\n",
    "            random_elements = random.randrange(1,Nim.rows[random_row] + 1)\n",
    "        else:\n",
    "            random_elements = random.randrange(1,min(Nim._k,Nim.rows[random_row])+1)\n",
    "\n",
    "          \n",
    "        return Nimply(random_row, random_elements) \n",
    "\n",
    "    def best_strategy_by_prof(self, state: Nim):\n",
    "        data = cook_status(state)\n",
    "        move  = next((bf for bf in data[\"brute_force\"] if bf[1] == 0), random.choice(data[\"brute_force\"]))[0]\n",
    "        return Nimply(move[0], move[1])\n",
    "\n",
    "    def evolvable_by_prof(self, state: Nim, p = 0.5):\n",
    "        data = cook_status(state)\n",
    "\n",
    "        if random.random() < p:\n",
    "            if state.k is not None:\n",
    "                ply = Nimply(data[\"shortest_row\"], min(state.k, random.randint(1, state.rows[data[\"shortest_row\"]])))\n",
    "            else:\n",
    "                ply = Nimply(data[\"shortest_row\"], random.randint(1, state.rows[data[\"shortest_row\"]]))\n",
    "\n",
    "        else:\n",
    "            if state.k is not None:\n",
    "                ply = Nimply(data[\"longest_row\"], min(state.k,random.randint(1, state.rows[data[\"longest_row\"]])))\n",
    "            else:\n",
    "                ply = Nimply(data[\"longest_row\"], random.randint(1, state.rows[data[\"longest_row\"]]))\n",
    "\n",
    "        return ply\n",
    "    \n",
    "    def evolvable_based_on_fixed_rules(self, state: Nim, cook_status: dict, alpha: float = 0.5, beta: float = 0.5):\n",
    "        initial_numbers = sum([i*2 + 1 for i in range(N_ROWS)])\n",
    "        actual_numbers = sum(state.rows)\n",
    "\n",
    "        # Early game strategy\n",
    "        if actual_numbers > alpha * initial_numbers:\n",
    "            \n",
    "            if cook_status['active_rows_number'] >= beta*N_ROWS:\n",
    "\n",
    "                row = cook_status[\"longest_row\"]\n",
    "                if state.k is not None:\n",
    "                    elements = min(state.k, state.rows[row])\n",
    "                    \n",
    "                else:\n",
    "                    elements = state.rows[row]\n",
    "                        \n",
    "                # state.nimming(row, elements)\n",
    "                return Nimply(row, elements)\n",
    "\n",
    "        row = cook_status[\"longest_row\"]\n",
    "\n",
    "        if cook_status[\"active_rows_number\"]%2 == 0 and state.rows[row]!=1 :\n",
    "            if state.k is not None:\n",
    "                #Leave at least one element\n",
    "                elements = min( (state.k - 1), state.rows[row] - 1)\n",
    "        \n",
    "            else:\n",
    "                #Leave at least one element\n",
    "                elements = state.rows[row] - 1\n",
    "\n",
    "        else:\n",
    "            if state.k is not None:\n",
    "                #Try to remove the maximum number of elements\n",
    "                elements = min(state.k, state.rows[row])\n",
    "            else:\n",
    "                #Try to remove the maximum number of elements\n",
    "                elements = state.rows[row]\n",
    "        \n",
    "        if elements == 0:\n",
    "            elements = 1\n",
    "        # state.nimming(row, elements)\n",
    "        return Nimply(row, elements)\n",
    "        \n",
    "    def evolvable_based_on_GA(self, state: Nim):\n",
    "        \n",
    "        # Population = possible moves\n",
    "        population = []\n",
    "        for _ in range(POPULATION_SIZE):\n",
    "            # temp_rows needed to evaluate nim_sum = fitness (low nim_sum is better!)\n",
    "            temp_rows = state.rows.copy()\n",
    "\n",
    "            # Choosing a random_row\n",
    "            nonzeroind = [i for i, e in enumerate(state.rows) if e != 0]\n",
    "            random_row = random.choice(nonzeroind)\n",
    "\n",
    "            #Choosing a random_move\n",
    "            if state.k is None:\n",
    "                random_elements = random.randrange(1, state.rows[random_row] + 1)\n",
    "\n",
    "            else:\n",
    "                random_elements =  min(random.randrange(1, state.rows[random_row] + 1), state.k)\n",
    "\n",
    "            temp_rows[random_row] -= random_elements\n",
    "            fitness = nim_sum(temp_rows)\n",
    "            pop = Move(random_row, random_elements, fitness)\n",
    "            population.append(pop)\n",
    "        \n",
    "\n",
    "        for g in range(N_GENERATIONS):\n",
    "            offspring = list()\n",
    "            for i in range(OFFSPRING_SIZE):\n",
    "\n",
    "                if random.random() < 0.5:\n",
    "                    # Selection of parents\n",
    "                    p = tournament(population.copy())\n",
    "\n",
    "                    # Offspring generation\n",
    "                    o = mutation(p, state)       \n",
    "\n",
    "                else:\n",
    "                    \n",
    "                    p1 = tournament(population)\n",
    "                    p2 = tournament(population)\n",
    "                    o = cross_over(p1, p2, state)\n",
    "\n",
    "                # Check if cross-over returned a valid solution.\n",
    "                # In this code, only valid solutions has been considered.\n",
    "                # Possible Improvement: Acceptance with penalties of non-valid solutions\n",
    "                if o == None:\n",
    "                    continue\n",
    "\n",
    "                offspring.append(o)\n",
    "            \n",
    "            # Adding new Offspings generated to Population list\n",
    "            population+=offspring\n",
    "            \n",
    "            # Sorting the Population, according to their fitness and selecting the firsts n_elements = POPULATION_SIZE\n",
    "            population = sorted(population, key=lambda i: i.fitness, reverse=False)[:POPULATION_SIZE]\n",
    "            logging.debug(f\"actual best {population[0]}\")\n",
    "        \n",
    "        return Nimply(population[0].row, population[0].num_objects)\n",
    "\n",
    "    # Internal function\n",
    "    def _minimax(self, state: Nim, maximizing: int = True, alpha=-1, beta=1, depth = 0):\n",
    "        \n",
    "        if depth == 100:\n",
    "            return False\n",
    "        # Check the result of the previous move\n",
    "        if all(item == 0 for item in state.rows):\n",
    "            # The player who made the previous move has already won\n",
    "            return -1 if maximizing else 1\n",
    "\n",
    "        cooked = cook_status(state)\n",
    "\n",
    "        scores = []\n",
    "        for new_state, move in cooked[\"possible_new_states\"]:\n",
    "            score = self._minimax(new_state, maximizing = not maximizing, alpha = alpha, beta = beta, depth= depth + 1)\n",
    "            if score != False:\n",
    "                scores.append(score)\n",
    "            if maximizing:\n",
    "                alpha = max(alpha, score)\n",
    "            else:\n",
    "                beta = min(beta, score)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "\n",
    "        return (max if maximizing else min)(scores)\n",
    "    \n",
    "    def min_max_best_move(self, state: Nim):\n",
    "        cooked = cook_status(state)\n",
    "        ply = None\n",
    "        for new_state, move in cooked[\"possible_new_states\"]:\n",
    "    \n",
    "            score = self._minimax(new_state, maximizing = False)\n",
    "\n",
    "            if score > 0:\n",
    "                ply = Nimply(move[0], move[1])\n",
    "                break\n",
    "        \n",
    "        if ply is None:\n",
    "            logging.debug(\" No winning moves :(\")\n",
    "            nonzeroind = [i for i, e in enumerate(state.rows) if e != 0]\n",
    "            random_row = random.choice(nonzeroind)\n",
    "\n",
    "            ply =  Nimply(random_row, 1)\n",
    "            \n",
    "        return ply\n",
    "\n",
    "    def rl_agent(self, state: Nim):\n",
    "        ply = self.agent.choose_action(state)\n",
    "        return ply\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reinforcement Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, state: Nim = None, alpha=0.15, random_factor=0.2, pretrained = False):  # 80% explore, 20% exploit\n",
    "        self.state_history = []  # state, reward\n",
    "        self.alpha = alpha\n",
    "        self.G = {}\n",
    "    \n",
    "        if pretrained:\n",
    "            with open('RL_agent.pkl', 'rb') as f:\n",
    "                self.G = pickle.load(f)\n",
    "            self.random_factor = 0\n",
    "            self.k = True\n",
    "        else:\n",
    "            assert state is not None, f'Please insert the starting state to inizialize rewards, if you want to train the agent. Otherwise, choose the `pretrained` option'\n",
    "            self.G = {}\n",
    "            self.init_reward(state)\n",
    "            self.random_factor = random_factor\n",
    "\n",
    "    \n",
    "    def init_reward(self, state: Nim):\n",
    "        cooked = cook_status(state)\n",
    "        for state, move in cooked[\"possible_new_states\"]:\n",
    "            self.G[tuple(state.rows)] =  np.random.uniform(low=-1.0, high=1.0)\n",
    "\n",
    "    def choose_action(self, state: Nim):\n",
    "\n",
    "        maxG = -10e15\n",
    "        next_move = None\n",
    "        cooked = cook_status(state)\n",
    "\n",
    "        randomN = np.random.random()\n",
    "        if randomN < self.random_factor:\n",
    "            # if random number below random factor, choose random action\n",
    "            moves = [move for new_state, move in cooked[\"possible_new_states\"]]\n",
    "            next_move = random.choice(moves)\n",
    "\n",
    "        else:\n",
    "            # if exploiting, gather all possible actions and choose one with the highest G (reward)\n",
    "            for new_state, move in cooked[\"possible_new_states\"]:\n",
    "                if tuple(new_state.rows) not in self.G:\n",
    "                    self.G[tuple(new_state.rows)] = np.random.uniform(low=-1.0, high=1.0)\n",
    "                    \n",
    "                if self.G[tuple(new_state.rows)] > maxG:\n",
    "    \n",
    "                    next_move = move\n",
    "                    next_move = Nimply(next_move[0], next_move[1])\n",
    "                    maxG = self.G[tuple(new_state.rows)]\n",
    "        \n",
    "        return Nimply(next_move[0], next_move[1])\n",
    "\n",
    "    def update_state_history(self, state, reward):\n",
    "        self.state_history.append((state, reward))\n",
    "    \n",
    "    def get_reward_from_state(self, state: Nim ):\n",
    "        return self.G[tuple(state.rows)]\n",
    "        \n",
    "    def learn(self):\n",
    "        prev = 0\n",
    "\n",
    "        for curr, reward in reversed(self.state_history):\n",
    "            if tuple(curr.rows) not in self.G:\n",
    "                self.G[tuple(curr.rows)] =  np.random.uniform(low=-1.0, high=1.0)\n",
    "            prev = reward + self.alpha * (prev)\n",
    "            self.G[tuple(curr.rows)] += prev\n",
    "\n",
    "        self.state_history = []\n",
    "        with open('RL_agent.pkl', 'wb') as f:\n",
    "            pickle.dump(self.G, f)\n",
    "        # np.save(os.path.join(os.getcwd(), 'lab3/RL_agent.npy'), self.G)\n",
    "\n",
    "        self.random_factor -= 10e-5  # decrease random factor each episode of play\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent_strategy = 'best', opponent_strategy = 'pure_random', parameter_dict: dict = {\"alpha\": None, \"beta\": None}, NUM_MATCHES = 10, random_board = True) -> float:\n",
    "    \n",
    "    won = 0\n",
    "    start = 0\n",
    "    \n",
    "    for m in tqdm(range(NUM_MATCHES)):\n",
    "\n",
    "        agent = Player(agent_strategy)\n",
    "        opponent = Player(opponent_strategy)\n",
    "\n",
    "        # 0 -> Agent's turn\n",
    "        # 1 -> Opponent's turn\n",
    "        turn = start\n",
    "\n",
    "        # the first move is equally distributed within matches\n",
    "        start = 1 - start \n",
    "\n",
    "        if random_board:\n",
    "            # Random board\n",
    "            rows = random.randint(1, 10)\n",
    "            K = random.randint(0, 2*rows)\n",
    "            if K == 0:\n",
    "                K = None\n",
    "        else:\n",
    "            rows = 3\n",
    "            K = 3\n",
    "\n",
    "        # Nim Table Creation    \n",
    "        nim = Nim(rows, K)\n",
    "\n",
    "        game_over = [0 for _ in range(rows)]\n",
    "\n",
    "        logging.debug(f\"\\n\\n\\n--------NEW GAME---------\")\n",
    "\n",
    "        # Game\n",
    "        while nim.rows != game_over:\n",
    "\n",
    "            if turn == 0:\n",
    "                logging.debug(f\" Actual turn: Agent\")\n",
    "            else:\n",
    "                logging.debug(f\" Actual turn: Opponent\")\n",
    "\n",
    "            logging.debug(f\" \\tTable before move: {nim} and Nim_sum: {nim_sum(nim._rows)}\")\n",
    "            \n",
    "            if turn == 0:\n",
    "                if agent_strategy == 'evolvable':\n",
    "                    assert parameter_dict['alpha'] is not None, f\"Please choose a value for alfa\"\n",
    "                    assert parameter_dict['beta'] is not None, f\"Please choose a value for beta\"\n",
    "                    ply = agent.moves(nim, parameter_dict['alpha'], parameter_dict['beta'] )\n",
    "                    \n",
    "\n",
    "                elif agent_strategy == 'evolvable_by_prof':\n",
    "                    assert parameter_dict['alpha'] is not None, f\"Please choose a value for alfa\"\n",
    "                    ply = agent.moves(nim, parameter_dict['alpha'])\n",
    "                else:\n",
    "                    ply = agent.moves(nim)\n",
    "                \n",
    "                logging.debug(f\" \\tAgent:   <Row: {ply.row}- Elements: {ply.num_objects}>\")\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                if opponent_strategy == 'evolvable':\n",
    "                    assert parameter_dict['alpha_opp'] is not None, f\"Please choose a value for alfa used by the opponent -> 'alpha_opp'\"\n",
    "                    assert parameter_dict['beta_opp'] is not None, f\"Please choose a value for beta used by the opponent -> 'beta_opp'\"\n",
    "                    ply = opponent.moves(nim, parameter_dict['alpha_opp'], parameter_dict['beta_opp'] )\n",
    "\n",
    "                elif opponent_strategy == 'evolvable_by_prof':\n",
    "                    assert parameter_dict['alpha_opp'] is not None, f\"Please choose a value for alfa used by the opponent -> 'alpha_opp'\"\n",
    "                    ply = opponent.moves(nim, parameter_dict['alpha_opp'])\n",
    "\n",
    "                else:\n",
    "                    ply = opponent.moves(nim)\n",
    "                \n",
    "                logging.debug(f\" \\tOpponent:   <Row: {ply.row}- Elements: {ply.num_objects}>\")\n",
    "            if ply.num_objects == 0:\n",
    "                print(f\"turn = {turn} \")\n",
    "            nim.nimming2(ply)\n",
    "            logging.debug(f\" \\tTable after move: {nim} and Nim_sum: {nim_sum(nim._rows)}\\n\")\n",
    "\n",
    "            \n",
    "            turn = 1 - turn\n",
    "        \n",
    "        logging.debug(f\"--------GAME OVER---------\")\n",
    "        # Game Over\n",
    "        if turn == 1:\n",
    "            won +=1\n",
    "        else:\n",
    "            logging.debug(f\"Game Lost by the agent is the n°{m}\")\n",
    "            \n",
    "        \n",
    "    return won / NUM_MATCHES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning of $\\alpha$ and $\\beta$ for taks 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "if TUNING:\n",
    "    max_win_rate = 0\n",
    "\n",
    "    # Value to test\n",
    "    values = list(x/10 for x in range(1, 10, 1))\n",
    "\n",
    "    parameter_dict= {}\n",
    "\n",
    "    for alpha in values:\n",
    "        for beta in values:\n",
    "        \n",
    "            parameter_dict[\"alpha\"] = alpha\n",
    "            parameter_dict[\"beta\"] = beta\n",
    "\n",
    "            act_won_rate = evaluate(agent_strategy='evolvable', opponent_strategy='pure_random', parameter_dict = parameter_dict)\n",
    "\n",
    "            if act_won_rate > max_win_rate:\n",
    "                alpha_best = alpha\n",
    "                beta_best = beta\n",
    "                max_win_rate = act_won_rate\n",
    "\n",
    "                logging.debug(f\" Found a new configuration:\\n\\talpha = {alpha_best}\\n\\tbeta = {beta_best}\\n\\twon rate = {max_win_rate}\")\n",
    "\n",
    "    \n",
    "    logging.info(f\"\\nBest configuration:\\n\\talpha = {alpha_best}\\n\\tbeta = {beta_best}\\n\\twon rate = {max_win_rate}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'randint'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/marcoprattico/Dropbox (Politecnico Di Torino Studenti)/PoliTo Magistrale/II Anno 2022-23/Computational Intelligence/computational_intellligence_22_23_294815/lab3/lab3_nim.ipynb Cella 22\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marcoprattico/Dropbox%20%28Politecnico%20Di%20Torino%20Studenti%29/PoliTo%20Magistrale/II%20Anno%202022-23/Computational%20Intelligence/computational_intellligence_22_23_294815/lab3/lab3_nim.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m parameter_dict[\u001b[39m\"\u001b[39m\u001b[39mbeta_opp\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marcoprattico/Dropbox%20%28Politecnico%20Di%20Torino%20Studenti%29/PoliTo%20Magistrale/II%20Anno%202022-23/Computational%20Intelligence/computational_intellligence_22_23_294815/lab3/lab3_nim.ipynb#X30sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Evaluation Section:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marcoprattico/Dropbox%20%28Politecnico%20Di%20Torino%20Studenti%29/PoliTo%20Magistrale/II%20Anno%202022-23/Computational%20Intelligence/computational_intellligence_22_23_294815/lab3/lab3_nim.ipynb#X30sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marcoprattico/Dropbox%20%28Politecnico%20Di%20Torino%20Studenti%29/PoliTo%20Magistrale/II%20Anno%202022-23/Computational%20Intelligence/computational_intellligence_22_23_294815/lab3/lab3_nim.ipynb#X30sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# print(f\"Agent Won: {evaluate()*100}% of the games\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marcoprattico/Dropbox%20%28Politecnico%20Di%20Torino%20Studenti%29/PoliTo%20Magistrale/II%20Anno%202022-23/Computational%20Intelligence/computational_intellligence_22_23_294815/lab3/lab3_nim.ipynb#X30sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# print(f\"Agent Won: {evaluate(agent_strategy='evolvable', opponent_strategy='evolvable', parameter_dict = parameter_dict)*100}% of the games\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marcoprattico/Dropbox%20%28Politecnico%20Di%20Torino%20Studenti%29/PoliTo%20Magistrale/II%20Anno%202022-23/Computational%20Intelligence/computational_intellligence_22_23_294815/lab3/lab3_nim.ipynb#X30sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# print(f\"Agent Won: {evaluate(agent_strategy='evolvable', opponent_strategy='pure_random', parameter_dict = parameter_dict)*100}% of the games\")\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/marcoprattico/Dropbox%20%28Politecnico%20Di%20Torino%20Studenti%29/PoliTo%20Magistrale/II%20Anno%202022-23/Computational%20Intelligence/computational_intellligence_22_23_294815/lab3/lab3_nim.ipynb#X30sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAgent Won: \u001b[39m\u001b[39m{\u001b[39;00mevaluate(agent_strategy\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrl\u001b[39m\u001b[39m'\u001b[39m, opponent_strategy\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpure_random\u001b[39m\u001b[39m'\u001b[39m, parameter_dict \u001b[39m=\u001b[39m parameter_dict)\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m% of the games\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/marcoprattico/Dropbox (Politecnico Di Torino Studenti)/PoliTo Magistrale/II Anno 2022-23/Computational Intelligence/computational_intellligence_22_23_294815/lab3/lab3_nim.ipynb Cella 22\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(agent_strategy, opponent_strategy, parameter_dict, NUM_MATCHES, random)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marcoprattico/Dropbox%20%28Politecnico%20Di%20Torino%20Studenti%29/PoliTo%20Magistrale/II%20Anno%202022-23/Computational%20Intelligence/computational_intellligence_22_23_294815/lab3/lab3_nim.ipynb#X30sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m start \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m start \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marcoprattico/Dropbox%20%28Politecnico%20Di%20Torino%20Studenti%29/PoliTo%20Magistrale/II%20Anno%202022-23/Computational%20Intelligence/computational_intellligence_22_23_294815/lab3/lab3_nim.ipynb#X30sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mif\u001b[39;00m random:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marcoprattico/Dropbox%20%28Politecnico%20Di%20Torino%20Studenti%29/PoliTo%20Magistrale/II%20Anno%202022-23/Computational%20Intelligence/computational_intellligence_22_23_294815/lab3/lab3_nim.ipynb#X30sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m# Random board\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/marcoprattico/Dropbox%20%28Politecnico%20Di%20Torino%20Studenti%29/PoliTo%20Magistrale/II%20Anno%202022-23/Computational%20Intelligence/computational_intellligence_22_23_294815/lab3/lab3_nim.ipynb#X30sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     rows \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39;49mrandint(\u001b[39m1\u001b[39m, \u001b[39m10\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marcoprattico/Dropbox%20%28Politecnico%20Di%20Torino%20Studenti%29/PoliTo%20Magistrale/II%20Anno%202022-23/Computational%20Intelligence/computational_intellligence_22_23_294815/lab3/lab3_nim.ipynb#X30sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     K \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m\u001b[39m*\u001b[39mrows)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marcoprattico/Dropbox%20%28Politecnico%20Di%20Torino%20Studenti%29/PoliTo%20Magistrale/II%20Anno%202022-23/Computational%20Intelligence/computational_intellligence_22_23_294815/lab3/lab3_nim.ipynb#X30sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mif\u001b[39;00m K \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bool' object has no attribute 'randint'"
     ]
    }
   ],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "parameter_dict= {}\n",
    "\n",
    "# Insert here your own parameters\n",
    "# Best values of alpha and beta against a pure random opponent are respectively 0.4 and 0.1 generally\n",
    "\n",
    "parameter_dict[\"alpha\"] = 0.4\n",
    "parameter_dict[\"beta\"] = 0.1\n",
    "parameter_dict[\"alpha_opp\"] = 0.99\n",
    "parameter_dict[\"beta_opp\"] = 0.1\n",
    "\n",
    "# Evaluation Section:\n",
    "\n",
    "# print(f\"Agent Won: {evaluate()*100}% of the games\")\n",
    "# print(f\"Agent Won: {evaluate(agent_strategy='best_prof')*100}% of the games\")\n",
    "# print(f\"Agent Won: {evaluate(agent_strategy='best', opponent_strategy='best_prof')*100}% of the games\")\n",
    "# print(f\"Agent Won: {evaluate(agent_strategy='ga', opponent_strategy='best_prof')*100}% of the games\")\n",
    "# print(f\"Agent Won: {evaluate(agent_strategy='evolvable', opponent_strategy='evolvable', parameter_dict = parameter_dict)*100}% of the games\")\n",
    "# print(f\"Agent Won: {evaluate(agent_strategy='evolvable', opponent_strategy='pure_random', parameter_dict = parameter_dict)*100}% of the games\")\n",
    "print(f\"Agent Won: {evaluate(agent_strategy='rl', opponent_strategy='pure_random', parameter_dict = parameter_dict)*100}% of the games\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Agent Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingRL():\n",
    "\n",
    "    # Nim Table Creation    \n",
    "    # This is the biggest table the RL agent can find and it is needed to inizialize the possible states\n",
    "    nim = Nim(10, None)\n",
    "    \n",
    "    agent = Agent(nim, alpha=0.8, random_factor=0.5)\n",
    "    \n",
    "    win_history_to_plot = []\n",
    "    indices = []\n",
    "\n",
    "    parameter_dict= {}\n",
    "    parameter_dict[\"alpha_opp\"] = 0.5\n",
    "    parameter_dict[\"beta_opp\"] = 0.5\n",
    "\n",
    "    start = 0\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "    for i in tqdm(range(5000)):\n",
    "\n",
    "        turn = 1 - start\n",
    "        start = turn\n",
    "        \n",
    "        opponent_strategy = 'best'\n",
    "        opponent = Player(opponent_strategy)\n",
    "\n",
    "        current_state = None\n",
    "        step = 0\n",
    "        \n",
    "        # Training the agent using DR\n",
    "        rows = random.randint(1, 10)\n",
    "        K = random.randint(0, 2*rows)\n",
    "        if K == 0:\n",
    "            K = None\n",
    "        # Nim Table Creation    \n",
    "        nim = Nim(rows, K)\n",
    "\n",
    "\n",
    "        game_over = [0 for _ in range(rows)]\n",
    "\n",
    "        while nim.rows != game_over:\n",
    "            if turn == 0:\n",
    "                logging.debug(f\" Actual turn: Agent\")\n",
    "            else:\n",
    "                logging.debug(f\" Actual turn: Opponent\")\n",
    "\n",
    "            logging.debug(f\" \\tTable before move: {nim} \")\n",
    "            step +=1\n",
    "            if turn == 0:\n",
    "                # Current state\n",
    "                current_state = nim\n",
    "                old_state =  nim\n",
    "                # Choose an action (explore or exploit)\n",
    "                ply = agent.choose_action(current_state)\n",
    "                logging.debug(f\" \\tAgent:   <Row: {ply.row}- Elements: {ply.num_objects}>\")\n",
    "                nim.nimming2(ply)\n",
    "\n",
    "                # New current state\n",
    "                current_state = deepcopy(nim)\n",
    "\n",
    "                # update the robot memory with state and reward\n",
    "                if current_state.rows == GAMEOVER:\n",
    "                    agent.update_state_history(current_state, 1)\n",
    "\n",
    "            else:\n",
    "\n",
    "                ply = opponent.moves(nim, parameter_dict['alpha_opp'], parameter_dict['beta_opp'] )\n",
    "                nim.nimming2(ply)\n",
    "\n",
    "                if current_state is not None:\n",
    "                    # update the robot memory with state and reward\n",
    "                    if nim.rows == GAMEOVER and turn == 1:\n",
    "                        agent.update_state_history(current_state, -1)\n",
    "                    else:\n",
    "                        agent.update_state_history(current_state, 0)\n",
    "\n",
    "            if step >= 1000:\n",
    "                break\n",
    "\n",
    "            logging.debug(f\" \\tTable after move: {nim} \")\n",
    "            turn = 1 - turn\n",
    "\n",
    "        agent.learn()  # robot should learn after every episode\n",
    "\n",
    "        # if i%5001 == 0:\n",
    "        #     # Game Over\n",
    "        #     if turn == 1:\n",
    "        #         # win_history_to_plot.append(0)\n",
    "        #         # indices.append(i)\n",
    "        #     else:\n",
    "        #         # win_history_to_plot.append(1)\n",
    "        #         # indices.append(i)\n",
    "\n",
    "    # plt.plot(indices, win_history_to_plot, \"b\", linewidth=1)\n",
    "    # plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RL_TRAINING:\n",
    "    trainingRL()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results For Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CODE_FOR_TABLE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/marcoprattico/Dropbox (Politecnico Di Torino Studenti)/PoliTo Magistrale/II Anno 2022-23/Computational Intelligence/computational_intellligence_22_23_294815/lab3/lab3_nim.ipynb Cella 27\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/marcoprattico/Dropbox%20%28Politecnico%20Di%20Torino%20Studenti%29/PoliTo%20Magistrale/II%20Anno%202022-23/Computational%20Intelligence/computational_intellligence_22_23_294815/lab3/lab3_nim.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m CODE_FOR_TABLE:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/marcoprattico/Dropbox%20%28Politecnico%20Di%20Torino%20Studenti%29/PoliTo%20Magistrale/II%20Anno%202022-23/Computational%20Intelligence/computational_intellligence_22_23_294815/lab3/lab3_nim.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     strategies \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mpure_random\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbest\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbest_prof\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mevolvable\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mevolvable_tuned\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mga\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mevolvable_prof\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrl\u001b[39m\u001b[39m'\u001b[39m ]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/marcoprattico/Dropbox%20%28Politecnico%20Di%20Torino%20Studenti%29/PoliTo%20Magistrale/II%20Anno%202022-23/Computational%20Intelligence/computational_intellligence_22_23_294815/lab3/lab3_nim.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     parameter_dict\u001b[39m=\u001b[39m {}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CODE_FOR_TABLE' is not defined"
     ]
    }
   ],
   "source": [
    "if CODE_FOR_TABLE:\n",
    "    strategies = ['pure_random', 'best', 'best_prof', 'evolvable', 'evolvable_tuned', 'ga','evolvable_prof', 'rl' ]\n",
    "    parameter_dict= {}\n",
    "    parameter_dict[\"alpha\"] = 0.5\n",
    "    parameter_dict[\"beta\"] = 0.5\n",
    "    parameter_dict[\"alpha_opp\"] = 0.5\n",
    "    parameter_dict[\"beta_opp\"] = 0.5\n",
    "    for agent in strategies:\n",
    "        for opponent in strategies:\n",
    "            print(f\"Agent Strategy: {agent} Opponent Strategy: {opponent} -> Agent Won: {evaluate(agent_strategy= agent, opponent_strategy=opponent, parameter_dict = parameter_dict, NUM_MATCHES=100, random_board=False)*100}% of the games \")\n",
    "            # print(f\"{agent} - {opponent} - {evaluate(agent_strategy= agent, opponent_strategy=opponent, parameter_dict = parameter_dict)*100}% of the games \")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
